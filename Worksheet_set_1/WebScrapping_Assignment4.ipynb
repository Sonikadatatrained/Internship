{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d029d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install beautifulsoup4 requests selenium\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3bda0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01e5697",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome()\n",
    "url= \"https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos\"\n",
    "try:\n",
    "    driver.get(url)\n",
    "    table = driver.find_element(By.XPATH,'//*[@id=\"mw-content-text\"]/div[1]/table[3]')\n",
    "    \n",
    "    ranks = []\n",
    "    names = []\n",
    "    artists = []\n",
    "    upload_dates = []\n",
    "    views = []\n",
    "\n",
    "    rows = table.find_elements(By.XPATH,'tr')\n",
    "    for row in rows[1:]:  \n",
    "        cols = row.find_elements(By.XPATH,'td')\n",
    "        try:\n",
    "            rank = int(cols[0].text)\n",
    "            name = cols[1].text\n",
    "            artist = cols[2].text\n",
    "            upload_date = cols[4].text\n",
    "            view = int(cols[5].text.replace(',', ''))\n",
    "            \n",
    "            ranks.append(rank)\n",
    "            names.append(name)\n",
    "            artists.append(artist)\n",
    "            upload_dates.append(upload_date)\n",
    "            views.append(view)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing row {rank}: {e}\")\n",
    "\n",
    "finally:\n",
    "   \n",
    "    driver.quit()\n",
    "\n",
    "\n",
    "for rank, name, artist, upload_date, view in zip(ranks, names, artists, upload_dates, views):\n",
    "    print(f\"Rank: {rank}, Name: {name}, Artist: {artist}, Upload Date: {upload_date}, Views: {view}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9915d4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome()\n",
    "try:\n",
    "    url = \"https://www.bcci.tv/\"\n",
    "    \n",
    "    driver.get(url)\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        cookie_notification = driver.find_element(By.CLASS_NAME, \"cookie\")\n",
    "        cookie_notification.click()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    \n",
    "    international_link = driver.find_element(By.LINK_TEXT, \"International\")\n",
    "    international_link.click()\n",
    "\n",
    "    \n",
    "    fixtures_link = driver.find_element(By.LINK_TEXT, \"Fixtures\")\n",
    "    fixtures_link.click()\n",
    "    \n",
    "    match_titles = []\n",
    "    series = []\n",
    "    places = []\n",
    "    dates = []\n",
    "    times = []\n",
    "    \n",
    "    fixture_items = driver.find_elements_by_class_name('js-list')\n",
    "\n",
    "    for item in fixture_items:\n",
    "        match_title = item.find_element_by_class_name('fixture__format').text\n",
    "        series_info = item.find_element_by_class_name('fixture__teams').text\n",
    "        place = item.find_element_by_class_name('fixture__info').text.split(',')[1].strip()\n",
    "        date = item.find_element_by_class_name('fixture__date').text\n",
    "        time = item.find_element_by_class_name('fixture__time').text\n",
    "\n",
    "        match_titles.append(match_title)\n",
    "        series.append(series_info)\n",
    "        places.append(place)\n",
    "        dates.append(date)\n",
    "        times.append(time)\n",
    "        \n",
    "finally:\n",
    "    driver.quit()\n",
    "\n",
    "for match_title, series_info, place, date, time in zip(match_titles, series, places, dates, times):\n",
    "    print(f\"Match Title: {match_title}, Series: {series_info}, Place: {place}, Date: {date}, Time: {time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b8bc91",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()\n",
    "\n",
    "try:\n",
    "    url = \"http://statisticstimes.com/\"\n",
    "    \n",
    "    driver.get(url)\n",
    "    economy_link = driver.find_element(By.XPATH,\"//div[@class='dropdown']/button[contains(text(), 'Economy')]\")\n",
    "    economy_link.click()\n",
    "    \n",
    "    india_link = driver.find_element(By.XPATH, \"//a[contains(text(), 'India')]\")\n",
    "    driver.execute_script(\"arguments[0].scrollIntoView();\", india_link)\n",
    "    india_link.click()\n",
    "    \n",
    "    gdp_link = driver.find_element(By.XPATH, \"//a[contains(text(), 'GDP of Indian states')]\")\n",
    "    driver.execute_script(\"arguments[0].scrollIntoView();\", gdp_link)\n",
    "    gdp_link.click()\n",
    "    \n",
    "\n",
    "    ranks = []\n",
    "    states = []\n",
    "    gsdp_18_19 = []\n",
    "    gsdp_19_20 = []\n",
    "    share_18_19 = []\n",
    "    gdp_billion = []\n",
    "    \n",
    "    table = driver.find_element(By.XPATH,\"//table[@id='table_id']\")\n",
    "    \n",
    "    rows = table.find_elements_by_tag_name('tr')\n",
    "    for row in rows[1:]:  \n",
    "        cols = row.find_elements_by_tag_name('td')\n",
    "        try:\n",
    "            rank = int(cols[0].text)\n",
    "            state = cols[1].text\n",
    "            gsd_18_19 = cols[2].text\n",
    "            gsd_19_20 = cols[3].text\n",
    "            share_18_19_value = cols[4].text\n",
    "            gdp_value = cols[5].text\n",
    "            \n",
    "            ranks.append(rank)\n",
    "            states.append(state)\n",
    "            gsdp_18_19.append(gsd_18_19)\n",
    "            gsdp_19_20.append(gsd_19_20)\n",
    "            share_18_19.append(share_18_19_value)\n",
    "            gdp_billion.append(gdp_value)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing row {rank}: {e}\")\n",
    "\n",
    "finally:\n",
    "   \n",
    "    driver.quit()\n",
    "\n",
    "\n",
    "for rank, state, gsd_18_19, gsd_19_20, share, gdp_value in zip(ranks, states, gsdp_18_19, gsdp_19_20, share_18_19, gdp_billion):\n",
    "    print(f\"Rank: {rank}, State: {state}, GSDP(18-19): {gsd_18_19}, GSDP(19-20): {gsd_19_20}, Share(18-19): {share}, GDP($ billion): {gdp_value}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fafbe046",
   "metadata": {},
   "source": [
    "driver = webdriver.Chrome()\n",
    "\n",
    "try:\n",
    "    url = \"https://github.com/\"\n",
    "    \n",
    "    driver.get(url)\n",
    "    \n",
    "    explore_dropdown = driver.find_element(By.XPATH,\"//summary[contains(text(), 'Explore')]\")\n",
    "    explore_dropdown.click()\n",
    "    \n",
    "    trending_link = driver.find_element(By.XPATH,\"//a[contains(text(), 'Trending')]\")\n",
    "    trending_link.click()\n",
    "    \n",
    "    repo_titles = []\n",
    "    repo_descriptions = []\n",
    "    contributors_counts = []\n",
    "    languages_used = []\n",
    "    \n",
    "     repo_items = driver.find_elements_by_css_selector('.Box-row')\n",
    "     \n",
    "     for item in repo_items:\n",
    "        try:\n",
    "            title = item.find_element_by_css_selector('h1.h3').text\n",
    "            description = item.find_element_by_css_selector('p-col-9').text\n",
    "            contributors = item.find_element_by_css_selector('.muted-link.mr-3').text\n",
    "            language = item.find_element_by_css_selector('[itemprop=\"programmingLanguage\"]').text\n",
    "\n",
    "            repo_titles.append(title)\n",
    "            repo_descriptions.append(description)\n",
    "            contributors_counts.append(contributors)\n",
    "            languages_used.append(language)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing repository: {e}\")\n",
    "            \n",
    "  finally:\n",
    "    \n",
    "    driver.quit()\n",
    "\n",
    "# Print or use the collected data as needed\n",
    "for title, description, contributors, language in zip(repo_titles, repo_descriptions, contributors_counts, languages_used):\n",
    "    print(f\"Repository Title: {title}\")\n",
    "    print(f\"Description: {description}\")\n",
    "    print(f\"Contributors Count: {contributors}\")\n",
    "    print(f\"Language Used: {language}\")\n",
    "    print(\"=\"*50)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed768dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()\n",
    "\n",
    "try:\n",
    "    url = \"https://www.billboard.com/\"\n",
    "    \n",
    "    driver.get(url)\n",
    "\n",
    "    \n",
    "    charts_option = driver.find_element(By.XPATH,\"//a[contains(text(), 'Charts')]\")\n",
    "    charts_option.click()\n",
    "\n",
    "    \n",
    "    hot_100_link = driver.find_element(By.XPATH,\"//a[contains(text(), 'Hot 100')]\")\n",
    "    hot_100_link.click()\n",
    "\n",
    "    \n",
    "    song_names = []\n",
    "    artist_names = []\n",
    "    last_week_ranks = []\n",
    "    peak_ranks = []\n",
    "    weeks_on_boards = []\n",
    "\n",
    "    \n",
    "    song_items = driver.find_elements_by_css_selector('.o-chart-results-list .c-chart-list')\n",
    "\n",
    "    for item in song_items:\n",
    "        try:\n",
    "            song_name = item.find_element_by_css_selector('.c-chart-list-item-title').text\n",
    "            artist_name = item.find_element_by_css_selector('.c-chart-list-item-artist').text\n",
    "            last_week_rank = item.find_element_by_css_selector('.c-chart-list-item-last-week').text\n",
    "            peak_rank = item.find_element_by_css_selector('.c-chart-list-item-peak-rank').text\n",
    "            weeks_on_board = item.find_element_by_css_selector('.c-chart-list-item-weeks-on-chart').text\n",
    "\n",
    "            song_names.append(song_name)\n",
    "            artist_names.append(artist_name)\n",
    "            last_week_ranks.append(last_week_rank)\n",
    "            peak_ranks.append(peak_rank)\n",
    "            weeks_on_boards.append(weeks_on_board)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing song: {e}\")\n",
    "\n",
    "finally:\n",
    "   \n",
    "    driver.quit()\n",
    "\n",
    "\n",
    "for song_name, artist_name, last_week_rank, peak_rank, weeks_on_board in zip(song_names, artist_names, last_week_ranks, peak_ranks, weeks_on_boards):\n",
    "    print(f\"Song Name: {song_name}\")\n",
    "    print(f\"Artist Name: {artist_name}\")\n",
    "    print(f\"Last Week Rank: {last_week_rank}\")\n",
    "    print(f\"Peak Rank: {peak_rank}\")\n",
    "    print(f\"Weeks on Board: {weeks_on_board}\")\n",
    "    print(\"=\"*50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be5aef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-grey-compare\"\n",
    "\n",
    "try:\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()  \n",
    "\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    novel_table = soup.find(\"table\", class_=\"in-article sortable\")\n",
    "\n",
    "    for row in novel_table.find_all(\"tr\")[1:]:\n",
    "        cols = row.find_all(\"td\")\n",
    "        book_name = cols[1].text\n",
    "        author_name = cols[2].text\n",
    "        volumes_sold = cols[3].text\n",
    "        publisher = cols[4].text\n",
    "        genre = cols[5].text\n",
    "\n",
    "        print(f\"Book Name: {book_name}\")\n",
    "        print(f\"Author Name: {author_name}\")\n",
    "        print(f\"Volumes Sold: {volumes_sold}\")\n",
    "        print(f\"Publisher: {publisher}\")\n",
    "        print(f\"Genre: {genre}\")\n",
    "        print(\"=\"*50)\n",
    "\n",
    "except requests.exceptions.HTTPError as http_err:\n",
    "    print(f\"HTTP error occurred: {http_err}\")\n",
    "except Exception as err:\n",
    "    print(f\"An error occurred: {err}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5cfd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.imdb.com/list/ls095964455/\"\n",
    "\n",
    "try:\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()  \n",
    "\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    series_list = soup.find_all(\"div\", class_=\"lister-item-content\")\n",
    "\n",
    "    for series in series_list:\n",
    "        name = series.find(\"h3\").a.text\n",
    "        year_span = series.find(\"span\", class_=\"lister-item-year\").text.strip(\"()\") if series.find(\"span\", class_=\"lister-item-year\") else None\n",
    "        genre = series.find(\"span\", class_=\"genre\").text.strip()\n",
    "        run_time = series.find(\"span\", class_=\"runtime\").text if series.find(\"span\", class_=\"runtime\") else None\n",
    "        ratings = series.find(\"span\", class_=\"ipl-rating-star__rating\").text if series.find(\"span\", class_=\"ipl-rating-star__rating\") else None\n",
    "        votes = series.find(\"span\", {\"name\": \"nv\"})[\"data-value\"] if series.find(\"span\", {\"name\": \"nv\"}) else None\n",
    "\n",
    "        print(f\"Name: {name}\")\n",
    "        print(f\"Year Span: {year_span}\")\n",
    "        print(f\"Genre: {genre}\")\n",
    "        print(f\"Run Time: {run_time}\")\n",
    "        print(f\"Ratings: {ratings}\")\n",
    "        print(f\"Votes: {votes}\")\n",
    "        print(\"=\"*50)\n",
    "\n",
    "except requests.exceptions.HTTPError as http_err:\n",
    "    print(f\"HTTP error occurred: {http_err}\")\n",
    "except Exception as err:\n",
    "    print(f\"An error occurred: {err}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d8af10",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://archive.ics.uci.edu/\"\n",
    "\n",
    "try:\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()  \n",
    "\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    \n",
    "    view_all_datasets_link = soup.find(\"a\", href=\"datasets.php\")\n",
    "    view_all_datasets_url = \"https://archive.ics.uci.edu/\" + view_all_datasets_link[\"href\"]\n",
    "    view_all_datasets_response = requests.get(view_all_datasets_url)\n",
    "    view_all_datasets_response.raise_for_status()\n",
    "\n",
    "    view_all_datasets_soup = BeautifulSoup(view_all_datasets_response.text, \"html.parser\")\n",
    "\n",
    "    dataset_table = view_all_datasets_soup.find(\"table\", class_=\"normal\")\n",
    "\n",
    "    for row in dataset_table.find_all(\"tr\")[1:]:\n",
    "        cols = row.find_all(\"td\")\n",
    "        dataset_name = cols[0].text.strip()\n",
    "        data_type = cols[1].text.strip()\n",
    "        task = cols[2].text.strip()\n",
    "        attribute_type = cols[3].text.strip()\n",
    "        num_instances = cols[4].text.strip()\n",
    "        num_attributes = cols[5].text.strip()\n",
    "        year = cols[6].text.strip()\n",
    "\n",
    "        print(f\"Dataset Name: {dataset_name}\")\n",
    "        print(f\"Data Type: {data_type}\")\n",
    "        print(f\"Task: {task}\")\n",
    "        print(f\"Attribute Type: {attribute_type}\")\n",
    "        print(f\"No. of Instances: {num_instances}\")\n",
    "        print(f\"No. of Attributes: {num_attributes}\")\n",
    "        print(f\"Year: {year}\")\n",
    "        print(\"=\"*50)\n",
    "\n",
    "except requests.exceptions.HTTPError as http_err:\n",
    "    print(f\"HTTP error occurred: {http_err}\")\n",
    "except Exception as err:\n",
    "    print(f\"An error occurred: {err}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b36c169",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
