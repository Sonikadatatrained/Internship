{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0e9614d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\programdata\\anaconda3\\lib\\site-packages (4.11.1)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (2.28.1)\n",
      "Requirement already satisfied: selenium in c:\\users\\armaa\\appdata\\roaming\\python\\python39\\site-packages (4.11.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.3.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests) (2022.9.14)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\armaa\\appdata\\roaming\\python\\python39\\site-packages (from selenium) (0.22.2)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\armaa\\appdata\\roaming\\python\\python39\\site-packages (from selenium) (0.10.3)\n",
      "Requirement already satisfied: attrs>=20.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (21.4.0)\n",
      "Requirement already satisfied: sniffio in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: outcome in c:\\users\\armaa\\appdata\\roaming\\python\\python39\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.15.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc9 in c:\\users\\armaa\\appdata\\roaming\\python\\python39\\site-packages (from trio~=0.17->selenium) (1.1.3)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\armaa\\appdata\\roaming\\python\\python39\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from urllib3<1.27,>=1.21.1->requests) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\programdata\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\armaa\\appdata\\roaming\\python\\python39\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install beautifulsoup4 requests selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3153f01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112bcae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome()\n",
    "val=input(\"Enter the product to search for on Amazon: \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed28563",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome(r\"C:\\Users\\armaa\\Downloads\\chromedriver_win32\\chromedriver.exe\")\n",
    "\n",
    "driver.get(\"https://www.amazon.in/\")\n",
    "\n",
    "speaker_tags=driver.find_element(By.XPATH,\"//div[@class='nav-search-field ']//input\")\n",
    "speaker_tags.send.keys(\"guitar\")\n",
    "search=driver.find_elements(By.XPATH,\"//div[@class='nav-search-submit nav-sprite']//span\")\n",
    "search.click()\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a02c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scrape all product urls\n",
    "\n",
    "product_urls=[]\n",
    "start=0\n",
    "end=3\n",
    "for page in range(start,end):\n",
    "    url=driver.find_elements(By.XPATH,'//a[@class=\"a-link-normal s-underline-text s-underline-link text s-link-style a-text-normal\"]')\n",
    "    \n",
    "    for i in url:\n",
    "        product_urls.append(i.get_attribute(\"href\"))\n",
    "    nxt_button=driver.find_elements(By.XPATH,'//a[@class=\"s-pagination-item s-pagination-next s-pagination-button s-pagination-separator\"]')\n",
    "  \n",
    "# Check if there is a next button\n",
    "   \n",
    "    if nxt_button:\n",
    "        nxt_button[0].click()\n",
    "        time.sleep(2)\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada5591b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(product_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba1590e",
   "metadata": {},
   "outputs": [],
   "source": [
    "brands = []  # Create a list to store the brands\n",
    "\n",
    "for url in product_urls:\n",
    "    driver.get(url)\n",
    "    time.sleep(2)\n",
    "    try:\n",
    "        \n",
    "        brand_element = driver.find_element(By.XPATH, '//*[@id=\"product-Overview-feature-div\"]/div/table/tbody/tr[i]/td/span')\n",
    "        brands.append(brand_element.text)\n",
    "    except NoSuchElementException:\n",
    "        brands.append('-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93d0280",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(\"https://images.google.com/\")\n",
    "\n",
    "# Scroll down to load more images\n",
    "for _ in range(20):\n",
    "    driver.execute_script(\"window.scrollBy(0,1000)\")\n",
    "\n",
    "# Find the image elements\n",
    "images = driver.find_elements(By.XPATH, '//img[@class=\"rg_i Q4LuWd\"]')\n",
    "\n",
    "img_urls = []\n",
    "\n",
    "# Get the source URLs of the images\n",
    "for image in images:\n",
    "    source = image.get_attribute('src')\n",
    "    if source is not None and source.startswith('http'):\n",
    "        img_urls.append(source)\n",
    "\n",
    "# Check if there are at least 10 images available\n",
    "if len(img_urls) >= 10:\n",
    "    # Download the first 10 images\n",
    "    for i in range(10):\n",
    "        print(f\"Downloading image {i+1} of 10\")\n",
    "        response = requests.get(img_urls[i])\n",
    "        with open(f\"image_{i+1}.jpg\", \"wb\") as file:\n",
    "            file.write(response.content)\n",
    "else:\n",
    "    print(\"Not enough images available for download.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf452be",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()\n",
    "\n",
    "    \n",
    "    driver.get('https://www.google.com/maps')\n",
    "    \n",
    "    search_bar = driver.find_element(By.XPATH, '//input[@id=\"searchboxinput\"]')\n",
    "    search_bar.send_keys(city_name)\n",
    "    search_bar.submit()\n",
    "    \n",
    "    driver.implicitly_wait(10)\n",
    "    page_source = driver.page_source\n",
    "    \n",
    "    soup = BeautifulSoup(page_source, 'html.parser')\n",
    "    \n",
    "    try:\n",
    "        url_string=driver.current_url\n",
    "        print(\"URL Extracted\", url_string)\n",
    "        lat_lng = re.findall(r'r@(.*)data', url_string)\n",
    "        driver.quit()\n",
    "        \n",
    "        return lat_lng\n",
    "    \n",
    "    city_name = input(\"Enter the name of the city: \")\n",
    "\n",
    "\n",
    "get_coordinates(city_name)\n",
    "\n",
    "    lat_lng = get_coordinates(city_name)\n",
    "    print(f\"Coordinates from URL: {lat_lng}\")\n",
    "    \n",
    "     if lat_lng:\n",
    "        lat_lng = lat_lng[0].split(\",\")\n",
    "        latitude = lat_lng[0]\n",
    "        longitude = lat_lng[1]\n",
    "        \n",
    "        print(f\"Latitude: {latitude}, Longitude: {longitude}\")\n",
    "    else:\n",
    "        print(\"Latitude and longitude not found in the URL.\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8c6668",
   "metadata": {},
   "outputs": [],
   "source": [
    "url='https://www.digit.in/top-products/best-gaming-laptops-40.html'\n",
    "response = requests.get(url)\n",
    "laptops = soup.find_all('div', class_='TopNumbeHeading active sticky-footer')\n",
    "\n",
    "for laptop in laptops:\n",
    "        name = laptop.h3.text.strip()\n",
    "        specs = laptop.find('div', class_='Specs').text.strip()\n",
    "        price = laptop.find('div', class_='Price').text.strip()\n",
    "        \n",
    "        print(f\"Name: {name}\\nSpecs: {specs}\\nPrice: {price}\\n\")\n",
    "else:\n",
    "    print(f\"Failed to retrieve the page. Status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bddfe0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.forbes.com/billionaires/'\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    \n",
    "    data = re.findall(r'<script id=\"__NEXT_DATA__\" type=\"application/json\">(.*?)</script>', response.text, re.DOTALL)\n",
    "    \n",
    "    if data:\n",
    "        data_json = json.loads(data[0])\n",
    "        billionaires = data_json['props']['initialState']['list']['billionaires']\n",
    "        \n",
    "        for billionaire in billionaires:\n",
    "            rank = billionaire['rank']\n",
    "            name = billionaire['person']['firstName'] + ' ' + billionaire['person']['lastName']\n",
    "            net_worth = billionaire['netWorth']\n",
    "            age = billionaire['age']\n",
    "            citizenship = billionaire['country']['citizenship']\n",
    "            source = billionaire['source']\n",
    "            industry = billionaire['source']['industry']\n",
    "            \n",
    "            print(f\"Rank: {rank}\\nName: {name}\\nNet Worth: {net_worth}\\nAge: {age}\\nCitizenship: {citizenship}\\nSource: {source}\\nIndustry: {industry}\\n\")\n",
    "    else:\n",
    "        print(\"Billionaire data not found in the page.\")\n",
    "else:\n",
    "    print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8d008a",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()\n",
    "driver.get('https://www.youtube.com/watch?v=0KNk-Joi-NM&list=RD0KNk-Joi-NM&start_radio=1')\n",
    "for _ in range(1000):\n",
    "    driver.execute_script(\"window.scrollBy(0,1000)\")\n",
    "    time.sleep(1)\n",
    "    \n",
    "    Comment_elements = driver.find_elements_by_xpath('//div[@id=\"content\"]//yt-formatted-string[@id=\"content-text\"]')\n",
    "\n",
    "\n",
    "upvotes_elements = driver.find_elements_by_xpath('//div[@id=\"content\"]//span[@id=\"vote-count-middle\"]')\n",
    "timestamp_elements = driver.find_elements_by_xpath('//div[@id=\"content\"]//a[@id=\"published-time-text\"]')\n",
    "\n",
    "\n",
    "comments = [comment.text for comment in comment_elements]\n",
    "upvotes = [upvote.text for upvote in upvotes_elements]\n",
    "timestamps = [timestamp.text for timestamp in timestamp_elements]\n",
    "\n",
    "# Print the first 10 comments as an example\n",
    "for i in range(10):\n",
    "    print(f\"Comment: {comments[i]}\\nUpvotes: {upvotes[i]}\\nTimestamp: {timestamps[i]}\\n\")\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad9f916",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome(r\"C:\\Users\\armaa\\Downloads\\chromedriver_win32\\chromedriver.exe\")\n",
    "\n",
    "time.sleep(3)\n",
    "driver.get(\"https://www.hostelworld.com/\")\n",
    "london_tag=driver.find_element(By.XPATH'/html/body/div[3]/div/div/div[2]/div[i]/div/div/div[4]/div/div[2]/div/div[1]/div/div/ul')\n",
    "london_btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dad26af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
